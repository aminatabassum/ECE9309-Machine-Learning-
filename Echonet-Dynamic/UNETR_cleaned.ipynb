{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import echonet\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import click\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import skimage.draw\n",
    "import torchvision\n",
    "import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _video_collate_fn(x):\n",
    "    \"\"\"Collate function for Pytorch dataloader to merge multiple videos.\n",
    "\n",
    "    This function should be used in a dataloader for a dataset that returns\n",
    "    a video as the first element, along with some (non-zero) tuple of\n",
    "    targets. Then, the input x is a list of tuples:\n",
    "      - x[i][0] is the i-th video in the batch\n",
    "      - x[i][1] are the targets for the i-th video\n",
    "\n",
    "    This function returns a 3-tuple:\n",
    "      - The first element is the videos concatenated along the frames\n",
    "        dimension. This is done so that videos of different lengths can be\n",
    "        processed together (tensors cannot be \"jagged\", so we cannot have\n",
    "        a dimension for video, and another for frames).\n",
    "      - The second element is contains the targets with no modification.\n",
    "      - The third element is a list of the lengths of the videos in frames.\n",
    "    \"\"\"\n",
    "    video, target = zip(*x)  # Extract the videos and targets\n",
    "\n",
    "    # ``video'' is a tuple of length ``batch_size''\n",
    "    #   Each element has shape (channels=3, frames, height, width)\n",
    "    #   height and width are expected to be the same across videos, but\n",
    "    #   frames can be different.\n",
    "\n",
    "    # ``target'' is also a tuple of length ``batch_size''\n",
    "    # Each element is a tuple of the targets for the item.\n",
    "\n",
    "    i = list(map(lambda t: t.shape[1], video))  # Extract lengths of videos in frames\n",
    "\n",
    "    # This contatenates the videos along the the frames dimension (basically\n",
    "    # playing the videos one after another). The frames dimension is then\n",
    "    # moved to be first.\n",
    "    # Resulting shape is (total frames, channels=3, height, width)\n",
    "    video = torch.as_tensor(np.swapaxes(np.concatenate(video, 1), 0, 1))\n",
    "\n",
    "    # Swap dimensions (approximately a transpose)\n",
    "    # Before: target[i][j] is the j-th target of element i\n",
    "    # After:  target[i][j] is the i-th target of element j\n",
    "    target = zip(*target)\n",
    "\n",
    "    return video, target, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, train, optim, device):\n",
    "    \"\"\"Run one epoch of training/evaluation for segmentation.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to train/evaulate.\n",
    "        dataloder (torch.utils.data.DataLoader): Dataloader for dataset.\n",
    "        train (bool): Whether or not to train model.\n",
    "        optim (torch.optim.Optimizer): Optimizer\n",
    "        device (torch.device): Device to run on\n",
    "    \"\"\"\n",
    "\n",
    "    total = 0.\n",
    "    n = 0\n",
    "\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    pos_pix = 0\n",
    "    neg_pix = 0\n",
    "\n",
    "    model.train(train)\n",
    "\n",
    "    large_inter = 0\n",
    "    large_union = 0\n",
    "    small_inter = 0\n",
    "    small_union = 0\n",
    "    large_inter_list = []\n",
    "    large_union_list = []\n",
    "    small_inter_list = []\n",
    "    small_union_list = []\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "            for (_, (large_frame, small_frame, large_trace, small_trace)) in dataloader:\n",
    "                # Count number of pixels in/out of human segmentation\n",
    "                pos += (large_trace == 1).sum().item()\n",
    "                pos += (small_trace == 1).sum().item()\n",
    "                neg += (large_trace == 0).sum().item()\n",
    "                neg += (small_trace == 0).sum().item()\n",
    "\n",
    "                # Count number of pixels in/out of computer segmentation\n",
    "                pos_pix += (large_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                pos_pix += (small_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (large_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (small_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "\n",
    "                # print(\"Shape of large_frame before adding depth:\", large_frame.shape)\n",
    "                # print(\"Shape of small_frame before adding depth:\", small_frame.shape)\n",
    "                # print('shape of large trace before adding depth is:',large_trace.shape)\n",
    "                # print('shape of small trace before adding depth is:',small_trace.shape)\n",
    "\n",
    "                large_frame = large_frame.unsqueeze(2).expand(-1,-1,16,-1,-1)  # Add depth dimension\n",
    "                small_frame = small_frame.unsqueeze(2).expand(-1,-1,16,-1,-1)  # Add depth dimension\n",
    "                large_trace = large_trace.unsqueeze(1).unsqueeze(2).expand(-1,-1,16,-1,-1)  # Add depth dimension\n",
    "                small_trace = small_trace.unsqueeze(1).unsqueeze(2).expand(-1,-1,16,-1,-1)  # Add depth dimension\n",
    "\n",
    "                # print(\"Shape of large_frame after adding depth:\", large_frame.shape)\n",
    "                # print(\"Shape of small_frame after adding depth:\", small_frame.shape)\n",
    "                # print(\"Shape of large_trace after adding depth:\", large_trace.shape)\n",
    "                # print(\"Shape of small_trace after adding depth:\", small_trace.shape)\n",
    "\n",
    "\n",
    "                # Run prediction for diastolic frames and compute loss\n",
    "                large_frame = large_frame.to(device)\n",
    "                large_trace = large_trace.to(device)\n",
    "                y_large = model(large_frame)\n",
    "                loss_large = torch.nn.functional.binary_cross_entropy_with_logits(y_large[:, 0, :, :], large_trace.squeeze(dim=1), reduction=\"sum\")\n",
    "                # Compute pixel intersection and union between human and computer segmentations\n",
    "                large_inter += np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                large_union += np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                large_inter_list.extend(np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "                large_union_list.extend(np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                # Run prediction for systolic frames and compute loss\n",
    "                small_frame = small_frame.to(device)\n",
    "                small_trace = small_trace.to(device)\n",
    "                y_small = model(small_frame)\n",
    "                loss_small = torch.nn.functional.binary_cross_entropy_with_logits(y_small[:, 0, :, :], small_trace.squeeze(dim=1), reduction=\"sum\")\n",
    "                # Compute pixel intersection and union between human and computer segmentations\n",
    "                small_inter += np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                small_union += np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                small_inter_list.extend(np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "                small_union_list.extend(np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                # Take gradient step if training\n",
    "                loss = (loss_large + loss_small) / 2\n",
    "                if train:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                # Accumulate losses and compute baselines\n",
    "                total += loss.item()\n",
    "                n += large_trace.size(0)\n",
    "                p = pos / (pos + neg)\n",
    "                p_pix = (pos_pix + 1) / (pos_pix + neg_pix + 2)\n",
    "\n",
    "                # Show info on process bar\n",
    "                pbar.set_postfix_str(\"{:.4f} ({:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(total / n / 112 / 112, loss.item() / large_trace.size(0) / 112 / 112, -p * math.log(p) - (1 - p) * math.log(1 - p), (-p_pix * np.log(p_pix) - (1 - p_pix) * np.log(1 - p_pix)).mean(), 2 * large_inter / (large_union + large_inter), 2 * small_inter / (small_union + small_inter)))\n",
    "                pbar.update()\n",
    "\n",
    "    large_inter_list = np.array(large_inter_list)\n",
    "    large_union_list = np.array(large_union_list)\n",
    "    small_inter_list = np.array(small_inter_list)\n",
    "    small_union_list = np.array(small_union_list)\n",
    "\n",
    "    return (total / n / 112 / 112,\n",
    "            large_inter_list,\n",
    "            large_union_list,\n",
    "            small_inter_list,\n",
    "            small_union_list,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation(\n",
    "    data_dir=None,\n",
    "    output=None,\n",
    "\n",
    "    model_name=\"deeplabv3_resnet50\",\n",
    "    model = None,\n",
    "    pretrained=False,\n",
    "    weights=None,\n",
    "\n",
    "    run_test=False,\n",
    "    save_video=False,\n",
    "    num_epochs=50,\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-5,\n",
    "    lr_step_period=None,\n",
    "    num_train_patients=None,\n",
    "    num_workers=4,\n",
    "    batch_size=20,\n",
    "    device=None,\n",
    "    seed=0,\n",
    "    target_transform = None,\n",
    "):\n",
    "    \"\"\"Trains/tests segmentation model.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str, optional): Directory containing dataset. Defaults to\n",
    "            `echonet.config.DATA_DIR`.\n",
    "        output (str, optional): Directory to place outputs. Defaults to\n",
    "            output/segmentation/<model_name>_<pretrained/random>/.\n",
    "        model_name (str, optional): Name of segmentation model. One of ``deeplabv3_resnet50'',\n",
    "            ``deeplabv3_resnet101'', ``fcn_resnet50'', or ``fcn_resnet101''\n",
    "            (options are torchvision.models.segmentation.<model_name>)\n",
    "            Defaults to ``deeplabv3_resnet50''.\n",
    "        model (str, optional): Name of model from timm library. Defaults to None.\n",
    "        pretrained (bool, optional): Whether to use pretrained weights for model\n",
    "            Defaults to False.\n",
    "        weights (str, optional): Path to checkpoint containing weights to\n",
    "            initialize model. Defaults to None.\n",
    "        run_test (bool, optional): Whether or not to run on test.\n",
    "            Defaults to False.\n",
    "        save_video (bool, optional): Whether to save videos with segmentations.\n",
    "            Defaults to False.\n",
    "        num_epochs (int, optional): Number of epochs during training\n",
    "            Defaults to 50.\n",
    "        lr (float, optional): Learning rate for SGD\n",
    "            Defaults to 1e-5.\n",
    "        weight_decay (float, optional): Weight decay for SGD\n",
    "            Defaults to 0.\n",
    "        lr_step_period (int or None, optional): Period of learning rate decay\n",
    "            (learning rate is decayed by a multiplicative factor of 0.1)\n",
    "            Defaults to math.inf (never decay learning rate).\n",
    "        num_train_patients (int or None, optional): Number of training patients\n",
    "            for ablations. Defaults to all patients.\n",
    "        num_workers (int, optional): Number of subprocesses to use for data\n",
    "            loading. If 0, the data will be loaded in the main process.\n",
    "            Defaults to 4.\n",
    "        device (str or None, optional): Name of device to run on. Options from\n",
    "            https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device\n",
    "            Defaults to ``cuda'' if available, and ``cpu'' otherwise.\n",
    "        batch_size (int, optional): Number of samples to load per batch\n",
    "            Defaults to 20.\n",
    "        seed (int, optional): Seed for random number generator. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seed RNGs\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set default output directory\n",
    "    if output is None:\n",
    "        output = os.path.join(\"output\", \"segmentation\", \"{}_{}\".format(model_name, \"pretrained\" if pretrained else \"random\"))\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    # Set device for computations\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set up model\n",
    "    if model is None:\n",
    "        model = torchvision.models.segmentation.__dict__[model_name](pretrained=pretrained, aux_loss=False)\n",
    "        model.classifier[-1] = torch.nn.Conv2d(model.classifier[-1].in_channels, 1, kernel_size=model.classifier[-1].kernel_size)  # change number of outputs to 1\n",
    "    else:\n",
    "        model = model\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    if weights is not None:\n",
    "        checkpoint = torch.load(weights)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Set up optimizer\n",
    "    #optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    if lr_step_period is None:\n",
    "        lr_step_period = math.inf\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_step_period)\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean, std = echonet.utils.get_mean_and_std(echonet.datasets.Echo(root=data_dir, split=\"train\"))\n",
    "    tasks = [\"LargeFrame\", \"SmallFrame\", \"LargeTrace\", \"SmallTrace\"]\n",
    "    kwargs = {\"target_type\": tasks,\n",
    "              \"mean\": mean,\n",
    "              \"std\": std\n",
    "              }\n",
    "\n",
    "    # Set up datasets and dataloaders\n",
    "    dataset = {}\n",
    "    dataset[\"train\"] = echonet.datasets.Echo(root=data_dir, split=\"train\", **kwargs)\n",
    "    if num_train_patients is not None and len(dataset[\"train\"]) > num_train_patients:\n",
    "        # Subsample patients (used for ablation experiment)\n",
    "        indices = np.random.choice(len(dataset[\"train\"]), num_train_patients, replace=False)\n",
    "        dataset[\"train\"] = torch.utils.data.Subset(dataset[\"train\"], indices)\n",
    "    dataset[\"val\"] = echonet.datasets.Echo(root=data_dir, split=\"val\", **kwargs)\n",
    "\n",
    "    # Run training and testing loops\n",
    "    with open(os.path.join(output, \"log.csv\"), \"a\") as f:\n",
    "        epoch_resume = 0\n",
    "        bestLoss = float(\"inf\")\n",
    "        try:\n",
    "            # Attempt to load checkpoint\n",
    "            checkpoint = torch.load(os.path.join(output, \"checkpoint.pt\"))\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['opt_dict'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_dict'])\n",
    "            epoch_resume = checkpoint[\"epoch\"] + 1\n",
    "            bestLoss = checkpoint[\"best_loss\"]\n",
    "            f.write(\"Resuming from epoch {}\\n\".format(epoch_resume))\n",
    "        except FileNotFoundError:\n",
    "            f.write(\"Starting run from scratch\\n\")\n",
    "\n",
    "        for epoch in range(epoch_resume, num_epochs):\n",
    "            print(\"Epoch #{}\".format(epoch), flush=True)\n",
    "            for phase in ['train', 'val']:\n",
    "                start_time = time.time()\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    torch.cuda.reset_peak_memory_stats(i)\n",
    "\n",
    "                ds = dataset[phase]\n",
    "                dataloader = torch.utils.data.DataLoader(\n",
    "                    ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=(device.type == \"cuda\"), drop_last=(phase == \"train\"))\n",
    "\n",
    "                loss, large_inter, large_union, small_inter, small_union = run_epoch(model, dataloader, phase == \"train\", optimizer, device)\n",
    "                overall_dice = 2 * (large_inter.sum() + small_inter.sum()) / (large_union.sum() + large_inter.sum() + small_union.sum() + small_inter.sum())\n",
    "                large_dice = 2 * large_inter.sum() / (large_union.sum() + large_inter.sum())\n",
    "                small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n",
    "                f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(epoch,\n",
    "                                                                    phase,\n",
    "                                                                    loss,\n",
    "                                                                    overall_dice,\n",
    "                                                                    large_dice,\n",
    "                                                                    small_dice,\n",
    "                                                                    time.time() - start_time,\n",
    "                                                                    large_inter.size,\n",
    "                                                                    sum(torch.cuda.max_memory_allocated() for i in range(torch.cuda.device_count())),\n",
    "                                                                    sum(torch.cuda.max_memory_reserved() for i in range(torch.cuda.device_count())),\n",
    "                                                                    batch_size))\n",
    "                f.flush()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Save checkpoint\n",
    "            save = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_loss': bestLoss,\n",
    "                'loss': loss,\n",
    "                'opt_dict': optimizer.state_dict(),\n",
    "                'scheduler_dict': scheduler.state_dict(),\n",
    "            }\n",
    "            torch.save(save, os.path.join(output, \"checkpoint.pt\"))\n",
    "            if loss < bestLoss:\n",
    "                torch.save(save, os.path.join(output, \"best.pt\"))\n",
    "                bestLoss = loss\n",
    "\n",
    "        # Load best weights\n",
    "        if num_epochs != 0:\n",
    "            checkpoint = torch.load(os.path.join(output, \"best.pt\"))\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            f.write(\"Best validation loss {} from epoch {}\\n\".format(checkpoint[\"loss\"], checkpoint[\"epoch\"]))\n",
    "\n",
    "        if run_test:\n",
    "            # Run on validation and test\n",
    "            for split in [\"val\", \"test\"]:\n",
    "                dataset = echonet.datasets.Echo(root=data_dir, split=split, **kwargs)\n",
    "                dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                         batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=(device.type == \"cuda\"))\n",
    "                loss, large_inter, large_union, small_inter, small_union = echonet.utils.segmentation.run_epoch(model, dataloader, False, None, device)\n",
    "\n",
    "                overall_dice = 2 * (large_inter + small_inter) / (large_union + large_inter + small_union + small_inter)\n",
    "                large_dice = 2 * large_inter / (large_union + large_inter)\n",
    "                small_dice = 2 * small_inter / (small_union + small_inter)\n",
    "                with open(os.path.join(output, \"{}_dice.csv\".format(split)), \"w\") as g:\n",
    "                    g.write(\"Filename, Overall, Large, Small\\n\")\n",
    "                    for (filename, overall, large, small) in zip(dataset.fnames, overall_dice, large_dice, small_dice):\n",
    "                        g.write(\"{},{},{},{}\\n\".format(filename, overall, large, small))\n",
    "\n",
    "                f.write(\"{} dice (overall): {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *echonet.utils.bootstrap(np.concatenate((large_inter, small_inter)), np.concatenate((large_union, small_union)), echonet.utils.dice_similarity_coefficient)))\n",
    "                f.write(\"{} dice (large):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *echonet.utils.bootstrap(large_inter, large_union, echonet.utils.dice_similarity_coefficient)))\n",
    "                f.write(\"{} dice (small):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *echonet.utils.bootstrap(small_inter, small_union, echonet.utils.dice_similarity_coefficient)))\n",
    "                f.flush()\n",
    "\n",
    "    # Saving videos with segmentations\n",
    "    dataset = echonet.datasets.Echo(root=data_dir, split=\"test\",\n",
    "                                    target_type=[\"Filename\", \"LargeIndex\", \"SmallIndex\"],  # Need filename for saving, and human-selected frames to annotate\n",
    "                                    mean=mean, std=std,  # Normalization\n",
    "                                    length=None, max_length=None, period=1  # Take all frames\n",
    "                                    )\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, num_workers=num_workers, shuffle=False, pin_memory=False, collate_fn=echonet.utils.segmentation._video_collate_fn)\n",
    "\n",
    "    # Save videos with segmentation\n",
    "    if save_video and not all(os.path.isfile(os.path.join(output, \"videos\", f)) for f in dataloader.dataset.fnames):\n",
    "        # Only run if missing videos\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        os.makedirs(os.path.join(output, \"videos\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output, \"size\"), exist_ok=True)\n",
    "        echonet.utils.latexify()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with open(os.path.join(output, \"size.csv\"), \"w\") as g:\n",
    "                g.write(\"Filename,Frame,Size,HumanLarge,HumanSmall,ComputerSmall\\n\")\n",
    "                for (x, (filenames, large_index, small_index), length) in tqdm.tqdm(dataloader):\n",
    "                    # Run segmentation model on blocks of frames one-by-one\n",
    "                    # The whole concatenated video may be too long to run together\n",
    "                    try:\n",
    "                        y = np.concatenate([model(x[i:(i + batch_size), :, :, :].to(device))[\"out\"].detach().cpu().numpy() for i in range(0, x.shape[0], batch_size)])\n",
    "                    except:\n",
    "                        y = np.concatenate([model(x[i:(i + batch_size), :, :, :].to(device)).detach().cpu().numpy() for i in range(0, x.shape[0], batch_size)])\n",
    "\n",
    "                    start = 0\n",
    "                    x = x.numpy()\n",
    "                    for (i, (filename, offset)) in enumerate(zip(filenames, length)):\n",
    "                        # Extract one video and segmentation predictions\n",
    "                        video = x[start:(start + offset), ...]\n",
    "                        logit = y[start:(start + offset), 0, :, :]\n",
    "\n",
    "                        # Un-normalize video\n",
    "                        video *= std.reshape(1, 3, 1, 1)\n",
    "                        video += mean.reshape(1, 3, 1, 1)\n",
    "\n",
    "                        # Get frames, channels, height, and width\n",
    "                        f, c, h, w = video.shape  # pylint: disable=W0612\n",
    "                        assert c == 3\n",
    "\n",
    "                        # Put two copies of the video side by side\n",
    "                        video = np.concatenate((video, video), 3)\n",
    "\n",
    "                        # If a pixel is in the segmentation, saturate blue channel\n",
    "                        # Leave alone otherwise\n",
    "                        video[:, 0, :, w:] = np.maximum(255. * (logit > 0), video[:, 0, :, w:])  # pylint: disable=E1111\n",
    "\n",
    "                        # Add blank canvas under pair of videos\n",
    "                        video = np.concatenate((video, np.zeros_like(video)), 2)\n",
    "\n",
    "                        # Compute size of segmentation per frame\n",
    "                        size = (logit > 0).sum((1, 2))\n",
    "\n",
    "                        # Identify systole frames with peak detection\n",
    "                        trim_min = sorted(size)[round(len(size) ** 0.05)]\n",
    "                        trim_max = sorted(size)[round(len(size) ** 0.95)]\n",
    "                        trim_range = trim_max - trim_min\n",
    "                        systole = set(scipy.signal.find_peaks(-size, distance=20, prominence=(0.50 * trim_range))[0])\n",
    "\n",
    "                        # Write sizes and frames to file\n",
    "                        for (frame, s) in enumerate(size):\n",
    "                            g.write(\"{},{},{},{},{},{}\\n\".format(filename, frame, s, 1 if frame == large_index[i] else 0, 1 if frame == small_index[i] else 0, 1 if frame in systole else 0))\n",
    "\n",
    "                        # Plot sizes\n",
    "                        fig = plt.figure(figsize=(size.shape[0] / 50 * 1.5, 3))\n",
    "                        plt.scatter(np.arange(size.shape[0]) / 50, size, s=1)\n",
    "                        ylim = plt.ylim()\n",
    "                        for s in systole:\n",
    "                            plt.plot(np.array([s, s]) / 50, ylim, linewidth=1)\n",
    "                        plt.ylim(ylim)\n",
    "                        plt.title(os.path.splitext(filename)[0])\n",
    "                        plt.xlabel(\"Seconds\")\n",
    "                        plt.ylabel(\"Size (pixels)\")\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(output, \"size\", os.path.splitext(filename)[0] + \".pdf\"))\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        # Normalize size to [0, 1]\n",
    "                        size -= size.min()\n",
    "                        size = size / size.max()\n",
    "                        size = 1 - size\n",
    "\n",
    "                        # Iterate the frames in this video\n",
    "                        for (f, s) in enumerate(size):\n",
    "\n",
    "                            # On all frames, mark a pixel for the size of the frame\n",
    "                            video[:, :, int(round(115 + 100 * s)), int(round(f / len(size) * 200 + 10))] = 255.\n",
    "\n",
    "                            if f in systole:\n",
    "                                # If frame is computer-selected systole, mark with a line\n",
    "                                video[:, :, 115:224, int(round(f / len(size) * 200 + 10))] = 255.\n",
    "\n",
    "                            def dash(start, stop, on=10, off=10):\n",
    "                                buf = []\n",
    "                                x = start\n",
    "                                while x < stop:\n",
    "                                    buf.extend(range(x, x + on))\n",
    "                                    x += on\n",
    "                                    x += off\n",
    "                                buf = np.array(buf)\n",
    "                                buf = buf[buf < stop]\n",
    "                                return buf\n",
    "                            d = dash(115, 224)\n",
    "\n",
    "                            if f == large_index[i]:\n",
    "                                # If frame is human-selected diastole, mark with green dashed line on all frames\n",
    "                                video[:, :, d, int(round(f / len(size) * 200 + 10))] = np.array([0, 225, 0]).reshape((1, 3, 1))\n",
    "                            if f == small_index[i]:\n",
    "                                # If frame is human-selected systole, mark with red dashed line on all frames\n",
    "                                video[:, :, d, int(round(f / len(size) * 200 + 10))] = np.array([0, 0, 225]).reshape((1, 3, 1))\n",
    "\n",
    "                            # Get pixels for a circle centered on the pixel\n",
    "                            r, c = skimage.draw.disk((int(round(115 + 100 * s)), int(round(f / len(size) * 200 + 10))), 4.1)\n",
    "\n",
    "                            # On the frame that's being shown, put a circle over the pixel\n",
    "                            video[f, :, r, c] = 255.\n",
    "\n",
    "                        # Rearrange dimensions and save\n",
    "                        video = video.transpose(1, 0, 2, 3)\n",
    "                        video = video.astype(np.uint8)\n",
    "                        echonet.utils.savevideo(os.path.join(output, \"videos\", filename), video, 50)\n",
    "\n",
    "                        # Move to next video\n",
    "                        start += offset\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'a4c-video-dir/'\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNETR\n",
    "# input should be (batch size, channels, depth, height,width)\n",
    "modelunetr = UNETR(\n",
    "    in_channels=3,  # Number of input channels (e.g., grayscale images)\n",
    "    out_channels=1,  # Number of output channels (e.g., binary segmentation)\n",
    "    img_size=(16, 112, 112),  # Input image size\n",
    "    feature_size=16,  # Feature size for the transformer\n",
    "    hidden_size=768,  # Hidden size for the transformer\n",
    "    mlp_dim=3072,  # MLP dimension\n",
    "    num_heads=12,  # Number of attention heads\n",
    "    #pos_embed=\"perceptron\",  # Positional embedding type\n",
    "    norm_name=\"instance\",  # Normalization type\n",
    "    dropout_rate=0.0,  # Dropout rate\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_path = 'EchoNet-Dynamic'\n",
    "# mean, std = echonet.utils.get_mean_and_std(echonet.datasets.Echo(root=file_path, split=\"train\"))\n",
    "# tasks = [\"LargeFrame\", \"SmallFrame\", \"LargeTrace\", \"SmallTrace\"]\n",
    "\n",
    "# kwargs = {\"target_type\": tasks,\n",
    "#           \"mean\": mean,\n",
    "#           \"std\": std\n",
    "#         }\n",
    "\n",
    "# train_dataset = echonet.datasets.Echo(root=file_path, split=\"train\", **kwargs)\n",
    "\n",
    "# #image, target = train_dataset[1]\n",
    "# dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers=0, shuffle=True, pin_memory=False, drop_last=False)\n",
    "\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     inputs = batch[0].to(device)  \n",
    "#     print(\"Input shape before model:\", inputs.shape)\n",
    "#     outputs = modelunetr(inputs)\n",
    "#     print(\"Output shape:\", outputs.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (_, (large_frame, small_frame, large_trace, small_trace)) in dataloader:\n",
    "#     print(large_frame.shape, small_frame.shape, large_trace.shape, small_trace.shape)\n",
    "#     large_frame=large_frame.unsqueeze(2).expand(-1,-1,16,-1,-1)\n",
    "#     print(large_frame.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataloader:\n",
    "#     large_frame = batch[0]  # Assuming batch[0] contains large_frame\n",
    "#     #large_frame = large_frame.unsqueeze(2)  # Add frames dimension\n",
    "#     print(\"Shape of large_frame:\", large_frame.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:20<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 93/373 [53:51<2:46:22, 35.65s/it, 8.4827 (7.1733) / 0.3115 0.1345, 0.2952, 0.3755]  C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\1353616503.py:98: RuntimeWarning: overflow encountered in scalar add\n",
      "  pbar.set_postfix_str(\"{:.4f} ({:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(total / n / 112 / 112, loss.item() / large_trace.size(0) / 112 / 112, -p * math.log(p) - (1 - p) * math.log(1 - p), (-p_pix * np.log(p_pix) - (1 - p_pix) * np.log(1 - p_pix)).mean(), 2 * large_inter / (large_union + large_inter), 2 * small_inter / (small_union + small_inter)))\n",
      " 34%|███▍      | 126/373 [1:12:39<2:27:28, 35.82s/it, 8.0342 (6.5120) / 0.3133 0.1347, -0.3529, -0.7654]C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\1353616503.py:80: RuntimeWarning: overflow encountered in scalar add\n",
      "  small_union += np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
      " 44%|████▍     | 165/373 [1:36:07<2:09:36, 37.39s/it, 7.6585 (6.2485) / 0.3132 0.1346, -0.6239, -1.9488]C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\1353616503.py:69: RuntimeWarning: overflow encountered in scalar add\n",
      "  large_union += np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
      " 56%|█████▌    | 208/373 [2:02:05<1:34:10, 34.24s/it, 7.3584 (6.2134) / 0.3131 0.1348, -1.1883, -36.8144]C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\1353616503.py:98: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  pbar.set_postfix_str(\"{:.4f} ({:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(total / n / 112 / 112, loss.item() / large_trace.size(0) / 112 / 112, -p * math.log(p) - (1 - p) * math.log(1 - p), (-p_pix * np.log(p_pix) - (1 - p_pix) * np.log(1 - p_pix)).mean(), 2 * large_inter / (large_union + large_inter), 2 * small_inter / (small_union + small_inter)))\n",
      "100%|██████████| 373/373 [3:39:04<00:00, 35.24s/it, 6.5837 (5.4324) / 0.3135 0.1346, -1.9637, 0.1525]     \n",
      "C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\2671098657.py:145: RuntimeWarning: overflow encountered in scalar add\n",
      "  overall_dice = 2 * (large_inter.sum() + small_inter.sum()) / (large_union.sum() + large_inter.sum() + small_union.sum() + small_inter.sum())\n",
      "C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\2671098657.py:146: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  large_dice = 2 * large_inter.sum() / (large_union.sum() + large_inter.sum())\n",
      "C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\2671098657.py:147: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n",
      "C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\2671098657.py:147: RuntimeWarning: overflow encountered in scalar add\n",
      "  small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n",
      "100%|██████████| 65/65 [13:58<00:00, 12.90s/it, 5.2466 (5.2446) / 0.3145 0.1385, 0.5605, 0.6396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 356/373 [3:35:42<08:39, 30.56s/it, 4.6747 (4.2130) / 0.3137 0.1348, -24.0271, 0.0046]    C:\\Users\\atabass4\\AppData\\Local\\Temp\\ipykernel_14672\\1353616503.py:79: RuntimeWarning: overflow encountered in scalar add\n",
      "  small_inter += np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
      "100%|██████████| 373/373 [3:45:46<00:00, 36.32s/it, 4.6524 (4.2577) / 0.3135 0.1346, -6.1131, -0.1204] \n",
      "100%|██████████| 65/65 [14:16<00:00, 13.18s/it, 4.1563 (4.1094) / 0.3145 0.1385, 0.5618, 0.6470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:40:12<00:00, 35.42s/it, 3.7164 (3.4146) / 0.3135 0.1346, -6.0509, -0.1728]   \n",
      "100%|██████████| 65/65 [13:03<00:00, 12.05s/it, 3.3492 (3.2464) / 0.3145 0.1385, 0.5597, 0.6416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:41:39<00:00, 35.65s/it, 3.0398 (2.7427) / 0.3135 0.1346, -6.4722, -0.1853]     \n",
      "100%|██████████| 65/65 [14:05<00:00, 13.01s/it, 2.7928 (2.7753) / 0.3145 0.1385, 0.5618, 0.6482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:43:38<00:00, 35.97s/it, 2.5245 (2.3057) / 0.3135 0.1346, -7.9646, -0.1725]    \n",
      "100%|██████████| 65/65 [13:24<00:00, 12.38s/it, 2.2912 (2.3050) / 0.3145 0.1385, 0.5499, 0.6346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:41:43<00:00, 35.67s/it, 2.0260 (1.7867) / 0.3135 0.1346, -12.8518, -0.1321]   \n",
      "100%|██████████| 65/65 [13:53<00:00, 12.82s/it, 1.8916 (1.8609) / 0.3145 0.1385, 0.5497, 0.6366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:45:22<00:00, 36.25s/it, 1.6908 (2.0475) / 0.3135 0.1346, -14.9334, -0.1304]   \n",
      "100%|██████████| 65/65 [14:13<00:00, 13.13s/it, 1.6129 (1.5171) / 0.3145 0.1385, 0.5526, 0.6405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:37:43<00:00, 35.02s/it, 1.4575 (1.3458) / 0.3135 0.1346, -17.1108, -0.1264]   \n",
      "100%|██████████| 65/65 [14:04<00:00, 12.99s/it, 1.4240 (1.3612) / 0.3145 0.1385, 0.5484, 0.6374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:46:16<00:00, 36.40s/it, 1.2689 (1.2132) / 0.3135 0.1346, -21.4673, -0.1201]   \n",
      "100%|██████████| 65/65 [14:29<00:00, 13.38s/it, 1.2605 (1.3348) / 0.3145 0.1385, 0.5537, 0.6411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [3:40:39<00:00, 35.49s/it, 1.1160 (1.0510) / 0.3135 0.1346, -25.7162, -0.1192]  \n",
      "100%|██████████| 65/65 [14:14<00:00, 13.14s/it, 1.1625 (1.0548) / 0.3145 0.1385, 0.5458, 0.6358]\n",
      "  0%|          | 0/65 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 3, 16, 16, 16], expected input[1, 20, 3, 112, 112] to have 3 channels, but got 20 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munetr\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelunetr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 188\u001b[0m, in \u001b[0;36mrun_segmentation\u001b[1;34m(data_dir, output, model_name, model, pretrained, weights, run_test, save_video, num_epochs, lr, weight_decay, lr_step_period, num_train_patients, num_workers, batch_size, device, seed, target_transform)\u001b[0m\n\u001b[0;32m    185\u001b[0m dataset \u001b[38;5;241m=\u001b[39m echonet\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mEcho(root\u001b[38;5;241m=\u001b[39mdata_dir, split\u001b[38;5;241m=\u001b[39msplit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset,\n\u001b[0;32m    187\u001b[0m                                          batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m(device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 188\u001b[0m loss, large_inter, large_union, small_inter, small_union \u001b[38;5;241m=\u001b[39m \u001b[43mechonet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m overall_dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (large_inter \u001b[38;5;241m+\u001b[39m small_inter) \u001b[38;5;241m/\u001b[39m (large_union \u001b[38;5;241m+\u001b[39m large_inter \u001b[38;5;241m+\u001b[39m small_union \u001b[38;5;241m+\u001b[39m small_inter)\n\u001b[0;32m    191\u001b[0m large_dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m large_inter \u001b[38;5;241m/\u001b[39m (large_union \u001b[38;5;241m+\u001b[39m large_inter)\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\OneDrive - The University of Western Ontario\\Documents\\dynamic\\echonet\\utils\\segmentation.py:409\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(model, dataloader, train, optim, device)\u001b[0m\n\u001b[0;32m    407\u001b[0m large_frame \u001b[38;5;241m=\u001b[39m large_frame\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    408\u001b[0m large_trace \u001b[38;5;241m=\u001b[39m large_trace\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 409\u001b[0m y_large \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlarge_frame\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    410\u001b[0m loss_large \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(y_large[:, \u001b[38;5;241m0\u001b[39m, :, :], large_trace, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# Compute pixel intersection and union between human and computer segmentations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:191\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\monai\\networks\\nets\\unetr.py:200\u001b[0m, in \u001b[0;36mUNETR.forward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_in):\n\u001b[1;32m--> 200\u001b[0m     x, hidden_states_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     enc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder1(x_in)\n\u001b[0;32m    202\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m hidden_states_out[\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\monai\\networks\\nets\\vit.py:122\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_token\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    124\u001b[0m         cls_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\monai\\networks\\blocks\\patchembedding.py:133\u001b[0m, in \u001b[0;36mPatchEmbeddingBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 133\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atabass4\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    719\u001b[0m     )\n\u001b[1;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 3, 16, 16, 16], expected input[1, 20, 3, 112, 112] to have 3 channels, but got 20 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'unetr'\n",
    "run_segmentati on(data_dir=file_path, model=modelunetr, num_epochs=num_epochs, model_name=model_name, output=f'output_{model_name}', run_test=True, save_video=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
